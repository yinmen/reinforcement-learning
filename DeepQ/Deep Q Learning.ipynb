{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "\n",
    "from collections import deque, namedtuple\n",
    "from lib import plotting\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-09-18 16:28:00,295] Making new env: SpaceInvaders-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"SpaceInvaders-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.train.SummaryWriter(summary_dir)\n",
    "\n",
    "    def preprocess_state(self, s):\n",
    "        # Crop the Atari image to 160x160 pixels\n",
    "        return s[:,34:-16,:,:]\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Placeholders for our input\n",
    "        self.X_pl = tf.placeholder(shape=[None, 160, 160, 3], dtype=tf.float32)\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        \n",
    "        batch_size = tf.shape(self.y_pl)[0]\n",
    "        \n",
    "        X_resized = tf.image.resize_images(self.X_pl, 84, 84)\n",
    "        X_grayscale = tf.to_float(tf.image.rgb_to_grayscale(X_resized))\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X_grayscale, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "        \n",
    "        # Get only the relevant predictions for the chosen actions\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "        \n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "        self.train_op = tf.contrib.layers.optimize_loss(\n",
    "            self.loss, tf.contrib.framework.get_global_step(), learning_rate=0.001, optimizer=\"Adam\")\n",
    "        \n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.merge_summary([\n",
    "                tf.scalar_summary(\"loss\", self.loss),\n",
    "                tf.histogram_summary(\"loss_hist\", self.losses),\n",
    "                tf.histogram_summary(\"q_values_hist\", self.predictions),\n",
    "                tf.scalar_summary(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "            ])\n",
    "        \n",
    "    \n",
    "    def predict(self, s):\n",
    "        sess = tf.get_default_session()\n",
    "        # Crop the image to 160x160 pixels\n",
    "        state = self.preprocess_state(s)\n",
    "        feed_dict = { self.X_pl: state }\n",
    "        return sess.run(self.predictions, feed_dict)\n",
    "    \n",
    "    def update(self, s, a, y):\n",
    "        sess = tf.get_default_session()\n",
    "        state = self.preprocess_state(s)\n",
    "        feed_dict = { self.X_pl: state, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         5.5256176  0.         0.       ]]\n",
      "379.421\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "e = Estimator(scope=\"aa\")\n",
    "with tf.Session() as sess:\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    observation = env.reset()\n",
    "    observations = np.expand_dims(observation, 0)\n",
    "    print(e.predict(observations))\n",
    "\n",
    "    # Example training step\n",
    "    y = np.array([33.0, 2.0])\n",
    "    a = np.array([1, 3])\n",
    "    print(e.update(np.array([observation] * 2), a, y))\n",
    "\n",
    "# tv = tf.trainable_variables()\n",
    "# v = tv[0]\n",
    "# print(v.name)\n",
    "# e.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def copy_model_parameters(estimator1, estimator2):\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "    # print([v.name for v in e1_params])\n",
    "    # print([v.name for v in e2_params])\n",
    "    # print([v.name for v in tf.trainable_variables()])\n",
    "    \n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "    # print(update_ops)\n",
    "    \n",
    "    sess = tf.get_default_session()\n",
    "    sess.run(update_ops)\n",
    "    \n",
    "\n",
    "# observation = env.reset()\n",
    "# tf.reset_default_graph()\n",
    "# e1 = Estimator(scope=\"e1\")\n",
    "# e2 = Estimator(scope=\"e2\")\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.initialize_all_variables())\n",
    "#     print(e1.predict(observations))\n",
    "#     print(e2.predict(observations))\n",
    "#     copy_model_parameters(e1, e2)\n",
    "#     print(e2.predict(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def deep_q_learning(env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    resume=True,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=5000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=8):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for fff-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        estimator: Action-Value function estimator\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Lambda time discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "        epsilon_decay: Each episode, epsilon is decayed by this factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    replay_memory = deque(maxlen=replay_memory_size)\n",
    "    \n",
    "    if not resume and os.path.exists(experiment_dir):\n",
    "        print(\"Not resuming and deleting {}.\".format(experiment_dir))\n",
    "        shutil.rmtree(experiment_dir)\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    total_t = 0\n",
    "    \n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "        \n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(tf.get_default_session(), latest_checkpoint)\n",
    "    \n",
    "    # Populate the replay memory with some random experience\n",
    "    print(\"Populating replay memory...\\n\")\n",
    "    state = env.reset()\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action = np.random.choice(len(VALID_ACTIONS))\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "        else:\n",
    "            state = next_state    \n",
    "    \n",
    "    env.monitor.start(monitor_path, resume=True)\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "                \n",
    "        # The policy we're following\n",
    "        policy = make_epsilon_greedy_policy(\n",
    "            q_estimator, epsilons[total_t] , len(VALID_ACTIONS))\n",
    "        \n",
    "        \n",
    "        # Reset the environment and pick the first action\n",
    "        state = env.reset()\n",
    "        \n",
    "        loss = None\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                copy_model_parameters(q_estimator, target_estimator)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "            \n",
    "            # env.render()\n",
    "            \n",
    "            # Print out which episode we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            # Take a step\n",
    "            action_probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            \n",
    "            # Save transition in replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # Sample from the replay memory\n",
    "            sample_len = min(batch_size, len(replay_memory))\n",
    "            sample_idx = np.random.choice(len(replay_memory), sample_len, replace=False)\n",
    "            samples = [replay_memory[_] for _ in sample_idx]\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "            \n",
    "            # Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "            \n",
    "            # TODO: Use action 4x at a time to speed up?\n",
    "            \n",
    "            # Perform gradient descent update\n",
    "            loss = q_estimator.update(states_batch, action_batch, targets_batch)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            total_t += 1\n",
    "            state = next_state\n",
    "        \n",
    "        \n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "        q_estimator.summary_writer.flush()\n",
    "\n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "        \n",
    "    env.monitor.close()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not resuming and deleting /Users/dennybritz/github/rl-tutorial/DeepQ/experiments/SpaceInvaders-v0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-09-18 17:53:46,839] VideoRecorder encoder exited with status 255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 0 (0) @ Episode 1/1000, loss: None"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [32] vs. [32,4]\n\t [[Node: q/OptimizeLoss/gradients/q/sub_grad/BroadcastGradientArgs = BroadcastGradientArgs[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](q/OptimizeLoss/gradients/q/sub_grad/Shape, q/OptimizeLoss/gradients/q/sub_grad/Shape_1)]]\nCaused by op 'q/OptimizeLoss/gradients/q/sub_grad/BroadcastGradientArgs', defined at:\n  File \"/Users/dennybritz/homebrew/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/dennybritz/homebrew/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2705, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2809, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2869, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-274-4009f15d6e54>\", line 10, in <module>\n    q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n  File \"<ipython-input-256-e1e6a3437d20>\", line 6, in __init__\n    self._build_model()\n  File \"<ipython-input-256-e1e6a3437d20>\", line 44, in _build_model\n    self.loss, tf.contrib.framework.get_global_step(), learning_rate=0.001, optimizer=\"Adam\")\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py\", line 154, in optimize_loss\n    gradients = opt.compute_gradients(loss, variables)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py\", line 478, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py\", line 510, in _SubGrad\n    rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 366, in _broadcast_gradient_args\n    name=name)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'q/sub', defined at:\n  File \"/Users/dennybritz/homebrew/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 19 identical lines from previous traceback]\n  File \"<ipython-input-256-e1e6a3437d20>\", line 6, in __init__\n    self._build_model()\n  File \"<ipython-input-256-e1e6a3437d20>\", line 41, in _build_model\n    self.losses = (self.y_pl - self.predictions)**2\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 754, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2224, in sub\n    result = _op_def_lib.apply_op(\"Sub\", x=x, y=y, name=name)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/homebrew/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/errors.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    449\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    451\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [32] vs. [32,4]\n\t [[Node: q/OptimizeLoss/gradients/q/sub_grad/BroadcastGradientArgs = BroadcastGradientArgs[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](q/OptimizeLoss/gradients/q/sub_grad/Shape, q/OptimizeLoss/gradients/q/sub_grad/Shape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-274-4009f15d6e54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                                     \u001b[0mupdate_target_estimator_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                     \u001b[0mexperiment_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                                     batch_size=32):\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTotal Steps: {}, Last Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-272-25c5736d3614>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(env, q_estimator, target_estimator, num_episodes, experiment_dir, resume, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;31m# Perform gradient descent update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-256-e1e6a3437d20>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, s, a, y)\u001b[0m\n\u001b[1;32m     70\u001b[0m         summaries, global_step, loss = sess.run(\n\u001b[1;32m     71\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             feed_dict)\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [32] vs. [32,4]\n\t [[Node: q/OptimizeLoss/gradients/q/sub_grad/BroadcastGradientArgs = BroadcastGradientArgs[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](q/OptimizeLoss/gradients/q/sub_grad/Shape, q/OptimizeLoss/gradients/q/sub_grad/Shape_1)]]\nCaused by op 'q/OptimizeLoss/gradients/q/sub_grad/BroadcastGradientArgs', defined at:\n  File \"/Users/dennybritz/homebrew/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/dennybritz/homebrew/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 442, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tornado/ioloop.py\", line 883, in start\n    handler_func(fd_obj, events)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 391, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 199, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2705, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2809, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2869, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-274-4009f15d6e54>\", line 10, in <module>\n    q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n  File \"<ipython-input-256-e1e6a3437d20>\", line 6, in __init__\n    self._build_model()\n  File \"<ipython-input-256-e1e6a3437d20>\", line 44, in _build_model\n    self.loss, tf.contrib.framework.get_global_step(), learning_rate=0.001, optimizer=\"Adam\")\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py\", line 154, in optimize_loss\n    gradients = opt.compute_gradients(loss, variables)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py\", line 478, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py\", line 510, in _SubGrad\n    rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 366, in _broadcast_gradient_args\n    name=name)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'q/sub', defined at:\n  File \"/Users/dennybritz/homebrew/Cellar/python3/3.5.1/Frameworks/Python.framework/Versions/3.5/lib/python3.5/runpy.py\", line 170, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 19 identical lines from previous traceback]\n  File \"<ipython-input-256-e1e6a3437d20>\", line 6, in __init__\n    self._build_model()\n  File \"<ipython-input-256-e1e6a3437d20>\", line 41, in _build_model\n    self.losses = (self.y_pl - self.predictions)**2\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\", line 754, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2224, in sub\n    result = _op_def_lib.apply_op(\"Sub\", x=x, y=y, name=name)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2310, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/dennybritz/venvs/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1232, in __init__\n    self._traceback = _extract_stack()\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())    \n",
    "    for t, stats in deep_q_learning(env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    num_episodes=1000,\n",
    "                                    resume=False,\n",
    "                                    replay_memory_init_size=1000,\n",
    "                                    update_target_estimator_every=1000,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    batch_size=32):\n",
    "        \n",
    "        print(\"\\nTotal Steps: {}, Last Reward: {}\".format(t, stats.episode_rewards[-1]))\n",
    "        \n",
    "        # Save plots\n",
    "        figures = plotting.plot_episode_stats(stats, noshow=True)\n",
    "        for fignum, fig in enumerate(figures):\n",
    "            fig.savefig(os.path.join(experiment_dir, \"fig_{}.png\".format(fignum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value {\n",
       "  simple_value: 2\n",
       "  node_name: \"episode_reward\"\n",
       "}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
