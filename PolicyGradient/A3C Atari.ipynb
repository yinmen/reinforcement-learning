{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import collections\n",
    "import multiprocessing\n",
    "import threading\n",
    "import time\n",
    "import shutil\n",
    "import unittest\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "  sys.path.append(\"./a3c\") \n",
    "from lib.envs.cliff_walking import CliffWalkingEnv\n",
    "from lib.atari.state_processor import StateProcessor\n",
    "from lib.atari import helpers as atari_helpers\n",
    "from lib import plotting\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"bb\"):\n",
    "    tf.scalar_summary(\"aa\", tf.constant(5.0))\n",
    "\n",
    "summary_ops = tf.get_collection(tf.GraphKeys.SUMMARIES)\n",
    "[s.name for s in summary_ops]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    return gym.envs.make(\"Breakout-v0\")\n",
    "\n",
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0, 1, 2, 3]\n",
    "\n",
    "# observation_space = make_env().observation_space\n",
    "# action_space = make_env().action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-01 15:47:29,020] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = make_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'estimators'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4c24ff5f438a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0ma3c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_copy_params_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPolicyMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dennybritz/github/rl/PolicyGradient/a3c/worker.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matari\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_processor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStateProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matari\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0matari_helpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mestimators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mValueEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPolicyEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mTransition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Transition\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"action\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"reward\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"next_state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"done\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'estimators'"
     ]
    }
   ],
   "source": [
    "from a3c.worker import make_copy_params_op\n",
    "\n",
    "class PolicyMonitor(object):\n",
    "    def __init__(env, policy_net, summary_writer):\n",
    "        self.env = env\n",
    "        self.global_policy_net = policy_net\n",
    "        self.summary_writer = summary_writer\n",
    "        self.sp = StateProcessor()\n",
    "        \n",
    "        with tf.variable_scope(\"policy_eval\"):\n",
    "            self.policy_net = PolicyEstimator(policy_net.num_outputs)\n",
    "        \n",
    "        self.video_dir = os.path.join(summary_writer.get_logdir(), \"videos\")\n",
    "        os.makedirs(self.video_dir)\n",
    "            \n",
    "        # Op to copy params from global policy/valuenets\n",
    "        self.copy_params_op = make_copy_params_op(\n",
    "            tf.contrib.slim.get_variables(scope=\"global\", collection=tf.GraphKeys.TRAINABLE_VARIABLES),\n",
    "            tf.contrib.slim.get_variables(scope=\"policy_eval\", collection=tf.GraphKeys.TRAINABLE_VARIABLES))            \n",
    "\n",
    "    def _policy_net_predict(self, state, sess):\n",
    "        feed_dict = { self.policy_net.states: [state] }\n",
    "        preds = sess.run(self.policy_net.predictions, feed_dict)\n",
    "        return preds[\"probs\"][0]\n",
    "    \n",
    "    def eval_once(self, sess):\n",
    "        # Copy params to local model\n",
    "        global_step, _ = sess.run([tf.contrib.framework.get_global_step(), self.copy_params_op])\n",
    "            \n",
    "        # Run an episode\n",
    "        \n",
    "        \n",
    "        done = False\n",
    "        state = atari_helpers.atari_make_initial_state(self.sp.process(self.env.reset()))\n",
    "        total_reward = 0.0\n",
    "        episode_length = 0\n",
    "        \n",
    "        while not done:\n",
    "            action_probs = self._policy_net_predict(self.state, sess)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            episode_length += 1\n",
    "            state = next_state\n",
    "\n",
    "        # Add summary\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=total_reward, tag=\"total_reward\")\n",
    "        episode_summary.value.add(simple_value=episode_length, tag=\"episode_length\")\n",
    "        self.summary_writer.add_summary(episode_summary, global_step)\n",
    "        self.summary_writer.flush()\n",
    "        \n",
    "        return total_reward, episode_length\n",
    "    \n",
    "    def continuous_eval(self, eval_every, sess):\n",
    "        while True:\n",
    "            self.eval_once(sess)\n",
    "            # Sleep until next evaluation cycle\n",
    "            time.sleep(eval_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/a3c'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_writer.get_logdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ValueEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-deb5bd2afb1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"global\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mvalue_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValueEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mpolicy_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ValueEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "T_MAX = 7\n",
    "NUM_WORKERS = 3\n",
    "\n",
    "# Model Directory\n",
    "model_dir = \"/tmp/a3c\"\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "os.makedirs(model_dir)\n",
    "a = tf.train.SummaryWriter(model_dir)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "with tf.variable_scope(\"global\") as vs:\n",
    "    value_net = ValueEstimator()\n",
    "    policy_net = PolicyEstimator(reuse=True)\n",
    "\n",
    "global_counter = itertools.count()\n",
    "\n",
    "workers = []\n",
    "for worker_id in range(NUM_WORKERS):\n",
    "    worker = Worker(\n",
    "        name=\"worker_{}\".format(worker_id),\n",
    "        policy_net=policy_net,\n",
    "        value_net=value_net,\n",
    "        global_counter=global_counter,\n",
    "        discount_factor = 0.99,\n",
    "        summary_writer=summary_writer)\n",
    "    workers.append(worker)\n",
    "    \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    \n",
    "    # Start workers\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_fn = lambda: worker.run(sess, coord, T_MAX)\n",
    "        t = threading.Thread(target=worker_fn)\n",
    "        t.start()\n",
    "        worker_threads.append(t) \n",
    "    \n",
    "    # Start update\n",
    "    # updater.run(sess, coord, MAX_UPDATE_STEPS)\n",
    "    \n",
    "    # Wait for all workers to finish\n",
    "    coord.join(worker_threads)\n",
    "    \n",
    "    # Close the queues\n",
    "    # policy_net_batcher.queue.close()\n",
    "    # value_net_batcher.queue.close()    \n",
    "    \n",
    "    # update_fn = lambda: updater.run(sess, coord, MAX_UPDATE_STEPS)\n",
    "    # threading.Thread(target=update_fn).start()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def actor_critic(env,\n",
    "#                  sess,\n",
    "#                  policy_net,\n",
    "#                  policy_net_batcher,\n",
    "#                  value_net,\n",
    "#                  value_net_batcher,\n",
    "#                  num_episodes,\n",
    "#                  discount_factor=0.99):\n",
    "#     \"\"\"\n",
    "#     Q-Learning algorithm for fff-policy TD control using Function Approximation.\n",
    "#     Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "    \n",
    "#     Args:\n",
    "#         env: OpenAI environment.\n",
    "#         estimator: Action-Value function estimator\n",
    "#         num_episodes: Number of episodes to run for.\n",
    "#         discount_factor: Lambda time discount factor.\n",
    "#         epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "#         epsilon_decay: Each episode, epsilon is decayed by this factor\n",
    "    \n",
    "#     Returns:\n",
    "#         An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Keeps track of useful statistics\n",
    "#     stats = plotting.EpisodeStats(\n",
    "#         episode_lengths=np.zeros(num_episodes),\n",
    "#         episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "#     Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "#     for i_episode in range(num_episodes):\n",
    "#         # print(\"Starting episode {}\".format(i_episode))\n",
    "#         # sys.stdout.flush()\n",
    "        \n",
    "#         # Reset the environment and pick the fisrst action\n",
    "#         state = env.reset()\n",
    "        \n",
    "#         episode = []\n",
    "        \n",
    "#         # One step in the environment\n",
    "#         for t in itertools.count():\n",
    "            \n",
    "#             # Take a step\n",
    "#             action_probs = policy_net.predict(state, sess=sess)\n",
    "#             action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "#             next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "#             # Keep track of the transition\n",
    "# #             episode.append(Transition(\n",
    "# #               state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "            \n",
    "#             # Update statistics\n",
    "#             stats.episode_rewards[i_episode] += reward\n",
    "#             stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "#             # Calculate TD Target\n",
    "#             value_next = value_net.predict(next_state, sess=sess)\n",
    "#             td_target = reward + discount_factor * value_next\n",
    "#             td_error = td_target - value_net.predict(state, sess=sess)\n",
    "            \n",
    "#             # Update the value estimator\n",
    "#             feed_dict = { value_net.states: [state], value_net.targets: [td_target] }\n",
    "#             value_net_batcher.add_grads(feed_dict, sess=sess)\n",
    "            \n",
    "#             # Update the policy estimator\n",
    "#             # using the td error as our advantage estimate\n",
    "#             feed_dict = { policy_net.states: [state], policy_net.targets: [td_error], policy_net.actions: [action]  }\n",
    "#             policy_net_batcher.add_grads(feed_dict, sess=sess)\n",
    "            \n",
    "#             if done:\n",
    "#                 break\n",
    "                \n",
    "#             state = next_state\n",
    "            \n",
    "#     return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_policy(env, sess, coord, estimator_policy, test_every):\n",
    "    global_step = tf.contrib.framework.get_global_step(graph=sess.graph)\n",
    "    try:\n",
    "        while not coord.should_stop():\n",
    "            time.sleep(test_every)\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "            num_steps = 0\n",
    "            while not done:\n",
    "                action_probs = estimator_policy.predict(state, sess=sess)\n",
    "                action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                num_steps += 1\n",
    "                total_reward += reward\n",
    "            \n",
    "            global_step_ = sess.run(global_step)\n",
    "            print(\"{}: Epsiode Finished. length: {}, reward: {}\".format(global_step_, num_steps, total_reward))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "    except tf.errors.CancelledError:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "\n",
    "# global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "# policy_net = PolicyEstimator()\n",
    "# policy_net_batcher = GradientBatcher(policy_net.optimizer, policy_net.loss, \"policy\")\n",
    "\n",
    "# value_net = ValueEstimator()\n",
    "# value_net_batcher = GradientBatcher(value_net.optimizer, value_net.loss, \"value\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    # threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    def gradient_worker_fn(coord):\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                policy_net_batcher.apply_grads_batch(8, sess=sess)\n",
    "                value_net_batcher.apply_grads_batch(8, sess=sess)\n",
    "        except tf.errors.CancelledError:\n",
    "            return\n",
    "        \n",
    "    # Start a new worker thread for each CPU\n",
    "    worker_coord = tf.train.Coordinator()\n",
    "    worker_fn = lambda: actor_critic(make_env(), sess, policy_net, policy_net_batcher, value_net, value_net_batcher, 100)\n",
    "    worker_threads = []\n",
    "    for i_cpu in range(multiprocessing.cpu_count()):\n",
    "        t = threading.Thread(target=worker_fn)\n",
    "        t.start()\n",
    "        worker_threads.append(t)\n",
    "    \n",
    "    # Start a thread that applies gradients\n",
    "    threading.Thread(target=lambda: gradient_worker_fn(worker_coord)).start()\n",
    "    \n",
    "    # Start a thread that evaluates\n",
    "    threading.Thread(target=lambda: test_policy(make_env(), sess, worker_coord, policy_net, 1)).start()\n",
    "    \n",
    "    # Wait for all workers to finish\n",
    "    worker_coord.join(worker_threads)\n",
    "    \n",
    "    # Close the queues\n",
    "    policy_net_batcher.queue.close()\n",
    "    value_net_batcher.queue.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting.plot_episode_stats(stats, smoothing_window=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
