## Model-Free Prediction & Control with Temporal Difference (TD) and Q-Learning


### Learning Goals

- Understand how to use the TD-0 algorithm for prediction
- Understand how to use SARSA for on-policy control
- Understand how to use Q-Learning for off-policy control
- Understand the benefits of TD algorithms over the MC approach
- Understand the backward and forward view of TD-Lambda


### Lectures & Readings

**Required:**

- David Silver's RL Course Lecture 4 - Model-Free Prediction ([video](https://www.youtube.com/watch?v=PnHCvfgC_ZA), [slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf))
- David Silver's RL Course Lecture 5 - Model-Free Control ([video](https://www.youtube.com/watch?v=0g4j2k_Ggc4), [slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/control.pdf))

**Optional:**

- [Reinforcement Learning: An Introduction](https://www.dropbox.com/s/b3psxv2r0ccmf80/book2015oct.pdf) - Chapter 6: Temporal-Difference Learning


### Exercises

- Implement the SARSA in Python
- Implement the Q-Learning in Python
- Implement SARSA-Lambda in Python using Eligibility Traces