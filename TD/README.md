## Model-Free Prediction & Control with Temporal Difference (TD) and Q-Learning


### Learning Goals

- Understand TD-0 for prediction
- Understand SARSA for on-policy control
- Understand Q-Learning for off-policy control
- Understand the benefits of TD algorithms over the MC/DP approach
- Understand the backward and forward view of TD-Lambda


### Lectures & Readings

**Required:**

- David Silver's RL Course Lecture 4 - Model-Free Prediction ([video](https://www.youtube.com/watch?v=PnHCvfgC_ZA), [slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf))
- David Silver's RL Course Lecture 5 - Model-Free Control ([video](https://www.youtube.com/watch?v=0g4j2k_Ggc4), [slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/control.pdf))

**Optional:**

- [Reinforcement Learning: An Introduction](https://www.dropbox.com/s/d6fyn4a5ag3atzk/bookdraft2016aug.pdf) - Chapter 6: Temporal-Difference Learning


### Exercises

- Windy Gridworld Environment Playground
- Implement SARSA in Python
- Implement Q-Learning in Python
- Implement SARSA-Lambda in Python using Eligibility Traces